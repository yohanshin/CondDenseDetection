defaults:
  - _self_
  - backbone: vitb
  - model: regressor_xCond
  - data: BEDLAM
  - optimizer: adamw_cosine
  - landmarks: verts_306

use_mask_encoder: false
embed_dim: ${backbone.embed_dim}
landmark_type: ${landmarks.type}
debug_mode: false
scale_image: 2
image_height_original: 256
image_width_original: 192
image_height: ${mult:${scale_image},${image_height_original}}
image_width: ${mult:${scale_image},${image_width_original}}
condition: ${model.condition}

image_size:
    - ${image_width}
    - ${image_height}

num_joints: ${landmarks.num_joints}
total_num_joints: ${landmarks.total_num_joints}
exp_name: ${model.name}-${backbone.name}-${landmarks.type}

gpus_n: 1
samples_per_gpu: 8
workers_per_gpu: ${if:${debug_mode}, 1, 10}
val_dataloader:
    samples_per_gpu: 32
test_dataloader:
    samples_per_gpu: 32

strategy: auto
log_steps: ${if:${debug_mode}, 1, 250}
total_epochs: 7
max_steps: ${div:300000,${gpus_n}}

loss_weights:
  joints2d: ${if:${debug_mode}, 0.15, 100.0}
  total: ${if:${debug_mode}, 0.5, 1.0}

data_cfg:
  image_size: ${image_size}

# Train config ---------------------------------------
work_dir: 'experiments'
log_level: logging.INFO
seed: 42
deterministic: True # whether not to evaluate the checkpoint during training
cudnn_benchmark: True # Use cudnn
resume_from: "" # CKPT path
launcher: 'none' # When distributed training ['none', 'pytorch', 'slurm', 'mpi']
use_amp: True
validate: True

autoscale_lr: True # automatically scale lr with the number of gpus

# Demo setting
image_path: ""
use_detection: False

dist_params:
  ...